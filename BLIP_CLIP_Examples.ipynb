{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972e315-9c50-434c-866c-e9a1638670f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to install the required packages first\n",
    "# pip install transformers torch torchvision pillow requests\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Example Image URL (ensure this URL points to a valid image)\n",
    "# image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/600px-PNG_transparency_demonstration_1.png\"\n",
    "# image_url =\"\\home\\amehmood\\Research\\oldCdrive\\ComputerVision\\AirPlane.jpg\"\n",
    "image_url =\"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/training_set/dogs/dog.64.jpg\"\n",
    "try:\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "    image = Image.open(BytesIO(response.content))  # Load image from response\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================\n",
    "# Demonstrate CLIP\n",
    "# ==========================\n",
    "# Prepare inputs for CLIP\n",
    "texts = [\"a logo\", \"a dog\", \"a cat\"]\n",
    "inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get CLIP predictions\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-to-text similarity\n",
    "    probs = logits_per_image.softmax(dim=1)  # Convert to probabilities\n",
    "\n",
    "print(\"CLIP Predictions:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Probability of '{text}': {probs[0][i].item():.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# Demonstrate BLIP\n",
    "# ==========================\n",
    "# Prepare inputs for BLIP\n",
    "blip_inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate captions\n",
    "with torch.no_grad():\n",
    "    generated_ids = blip_model.generate(**blip_inputs)\n",
    "    caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nBLIP Caption:\")\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883ac2cf-88e8-4793-8c14-bf62099a8f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 14:46:56.676467: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 14:46:56.678044: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 14:46:56.713351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 14:47:01.175979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Predictions:\n",
      "Probability of 'a logo': 0.0018\n",
      "Probability of 'a dog': 0.9938\n",
      "Probability of 'a cat': 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amehmood/anaconda3/envs/main_env/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLIP Caption:\n",
      "a dog is sitting in the grass with its owner\n"
     ]
    }
   ],
   "source": [
    "# Make sure to install the required packages first\n",
    "# pip install transformers torch torchvision pillow requests\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Update the image path to point to a file in Drive D via WSL\n",
    "image_path = \"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/training_set/dogs/dog.64.jpg\"\n",
    "\n",
    "# Open image using PIL\n",
    "try:\n",
    "    if os.path.exists(image_path):\n",
    "        image = Image.open(image_path)\n",
    "    else:\n",
    "        print(f\"Image file not found at {image_path}\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================\n",
    "# Demonstrate CLIP\n",
    "# ==========================\n",
    "# Prepare inputs for CLIP\n",
    "texts = [\"a logo\", \"a dog\", \"a cat\"]\n",
    "inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get CLIP predictions\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-to-text similarity\n",
    "    probs = logits_per_image.softmax(dim=1)  # Convert to probabilities\n",
    "\n",
    "print(\"CLIP Predictions:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Probability of '{text}': {probs[0][i].item():.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# Demonstrate BLIP\n",
    "# ==========================\n",
    "# Prepare inputs for BLIP\n",
    "blip_inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate captions\n",
    "with torch.no_grad():\n",
    "    generated_ids = blip_model.generate(**blip_inputs)\n",
    "    caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nBLIP Caption:\")\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63aed11a-0267-47a0-9a2b-ba349e0c1463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: dog.4137.jpg\n",
      "\n",
      "CLIP Predictions for dog.4137.jpg:\n",
      "Probability of 'a logo': 0.0006\n",
      "Probability of 'a dog': 0.9938\n",
      "Probability of 'a cat': 0.0057\n",
      "\n",
      "BLIP Caption for dog.4137.jpg: a dog sitting in the grass near a fence\n",
      "Processing image: dog.4116.jpg\n",
      "\n",
      "CLIP Predictions for dog.4116.jpg:\n",
      "Probability of 'a logo': 0.0005\n",
      "Probability of 'a dog': 0.8899\n",
      "Probability of 'a cat': 0.1097\n",
      "\n",
      "BLIP Caption for dog.4116.jpg: a small white dog standing next to a fence\n",
      "Processing image: WVlogo.jpg\n",
      "\n",
      "CLIP Predictions for WVlogo.jpg:\n",
      "Probability of 'a logo': 0.9985\n",
      "Probability of 'a dog': 0.0008\n",
      "Probability of 'a cat': 0.0007\n",
      "\n",
      "BLIP Caption for WVlogo.jpg: jaguar logo on a building\n",
      "Processing image: dog.4132.jpg\n",
      "\n",
      "CLIP Predictions for dog.4132.jpg:\n",
      "Probability of 'a logo': 0.0004\n",
      "Probability of 'a dog': 0.9956\n",
      "Probability of 'a cat': 0.0040\n",
      "\n",
      "BLIP Caption for dog.4132.jpg: a dog with a blue collar and a white nose\n",
      "Processing image: cat.4122.jpg\n",
      "\n",
      "CLIP Predictions for cat.4122.jpg:\n",
      "Probability of 'a logo': 0.0007\n",
      "Probability of 'a dog': 0.0048\n",
      "Probability of 'a cat': 0.9946\n",
      "\n",
      "BLIP Caption for cat.4122.jpg: a black cat sitting on top of a blanket\n",
      "Processing image: dog.4118.jpg\n",
      "\n",
      "CLIP Predictions for dog.4118.jpg:\n",
      "Probability of 'a logo': 0.0000\n",
      "Probability of 'a dog': 0.9975\n",
      "Probability of 'a cat': 0.0024\n",
      "\n",
      "BLIP Caption for dog.4118.jpg: a dog standing in front of a wall\n",
      "Processing image: dog.4154.jpg\n",
      "\n",
      "CLIP Predictions for dog.4154.jpg:\n",
      "Probability of 'a logo': 0.0034\n",
      "Probability of 'a dog': 0.9940\n",
      "Probability of 'a cat': 0.0026\n",
      "\n",
      "BLIP Caption for dog.4154.jpg: a dog standing on top of a green bench\n",
      "Processing image: dog.4133.jpg\n",
      "\n",
      "CLIP Predictions for dog.4133.jpg:\n",
      "Probability of 'a logo': 0.0000\n",
      "Probability of 'a dog': 0.9932\n",
      "Probability of 'a cat': 0.0067\n",
      "\n",
      "BLIP Caption for dog.4133.jpg: a small dog standing in a mulch\n",
      "Processing image: tesla.jpg\n",
      "\n",
      "CLIP Predictions for tesla.jpg:\n",
      "Probability of 'a logo': 0.9954\n",
      "Probability of 'a dog': 0.0038\n",
      "Probability of 'a cat': 0.0008\n",
      "\n",
      "BLIP Caption for tesla.jpg: a red tesla logo on a red background\n",
      "Processing image: dog.4129.jpg\n",
      "\n",
      "CLIP Predictions for dog.4129.jpg:\n",
      "Probability of 'a logo': 0.0004\n",
      "Probability of 'a dog': 0.9934\n",
      "Probability of 'a cat': 0.0062\n",
      "\n",
      "BLIP Caption for dog.4129.jpg: a dog laying on top of a bed\n"
     ]
    }
   ],
   "source": [
    "# Make sure to install the required packages first\n",
    "# pip install transformers torch torchvision pillow requests\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Define the path to the dataset folder\n",
    "dataset_path = \"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/test_set/imgs\"\n",
    "\n",
    "# Get a list of all image files in the dataset folder\n",
    "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "# Randomly select 10 images from the list\n",
    "random_image_files = random.sample(image_files, 10)\n",
    "\n",
    "# Loop through the randomly selected images\n",
    "for image_file in random_image_files:\n",
    "    image_path = os.path.join(dataset_path, image_file)\n",
    "    \n",
    "    try:\n",
    "        # Load the image using PIL\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        print(f\"Processing image: {image_file}\")\n",
    "        \n",
    "        # ==========================\n",
    "        # Demonstrate CLIP\n",
    "        # ==========================\n",
    "        # Prepare inputs for CLIP\n",
    "        texts = [\"a logo\", \"a dog\", \"a cat\"]  # You can modify the texts to match your dataset\n",
    "        inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Get CLIP predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image  # Image-to-text similarity\n",
    "            probs = logits_per_image.softmax(dim=1)  # Convert to probabilities\n",
    "\n",
    "        # Print CLIP results\n",
    "        print(f\"\\nCLIP Predictions for {image_file}:\")\n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"Probability of '{text}': {probs[0][i].item():.4f}\")\n",
    "\n",
    "        # ==========================\n",
    "        # Demonstrate BLIP\n",
    "        # ==========================\n",
    "        # Prepare inputs for BLIP\n",
    "        blip_inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate captions\n",
    "        with torch.no_grad():\n",
    "            generated_ids = blip_model.generate(**blip_inputs)\n",
    "            caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print BLIP results\n",
    "        print(f\"\\nBLIP Caption for {image_file}: {caption}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d929c-2315-4703-ba01-03515b0a46d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df4d52-292a-4848-943c-8c17e5590a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
