{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eff3d43-4d48-41c7-87fb-a4b4a1de4adf",
   "metadata": {},
   "\n source": [
    "# RAG using Llama 2, Langchain and ChromaDB\n",
    "The RAG (Retrieval-Augmented Generation) model leverages Llama 2, Langchain, and ChromaDB to enhance natural language understanding and generation tasks. Llama 2 is a retrieval model designed to efficiently handle large-scale retrieval tasks, enabling the RAG model to retrieve relevant information from vast datasets. Langchain facilitates multilingual support and cross-lingual knowledge transfer, enabling the RAG model to understand and generate text in multiple languages. ChromaDB provides access to a rich source of structured and unstructured data, enabling the RAG model to access diverse information for improved performance in various natural language processing tasks. By integrating these components, the RAG model achieves state-of-the-art performance in tasks such as question answering, text summarization, and conversation generation.\n",
    "<center>  <img src=\"./RAG1.png\"  alt=\"Markdown Monster icon\"\n",
    "                                         style=\"float: center; margin-right: 10px;\" /> </center>\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a3169-e319-4796-b988-98e96473f6fd",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Use Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM). When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c8050-f06f-4569-8cec-df64e557f8cc",
   "metadata": {},
   "source": [
    "Before we embark on the code, let's define some of the terms and/or explain the acronyms used in this tutorial:\n",
"\n Credit: (https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb) \n",

    "## Definitions\n",
    "- LLM - Large Language Model\n",
    "- Llama 2.0 - LLM from Meta\n",
    "- Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "- Vector database - a database that organizes data through high-dimmensional vectors\n",
    "- ChromaDB - vector database\n",
    "- RAG - Retrieval Augmented Generation (see below more details about RAGs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f145a02-a8db-4ce6-afb1-e4ad07eaf18b",
   "metadata": {},
   "source": [
    "## Model details\n",
    "- Model: Llama 2\n",
    "- Variation: 7b-chat-hf (7b: 7B dimm. hf: HuggingFace build)\n",
    "- Version: V1\n",
    "- Framework: PyTorch\n",
    "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f6236-8b60-4cd2-a7c8-a44948ee7517",
   "metadata": {},
   "source": [
    "# What is a Retrieval-Augmented Generation\n",
    "A Retrieval-Augmented Generation (RAG) system combines the capabilities of Large Language Models (LLMs) with external resources to enhance natural language understanding and generation tasks. While LLMs excel at providing accurate responses within their trained domain, they may struggle when confronted with unfamiliar topics. RAG addresses this limitation by integrating a retriever and a generator.\n",
    "\n",
    "The retriever component encodes the dataset to facilitate efficient retrieval of relevant information. This is achieved through text embeddings, which create vector representations of the data. Various options exist for implementing a retriever, including vector databases like ChromaDB, Mevius, FAISS, Pinecone, and Weaviate. In this Notebook, we'll utilize a local instance of ChromaDB.\n",
    "\n",
    "For the generator component, LLMs are the preferred choice. Specifically, we'll utilize a quantized LLaMA v2 model from the Kaggle Models collection.\n",
    "\n",
    "The orchestration of the retriever and generator is facilitated by Langchain. With specialized functions from Langchain, we can create the retriever-generator system in just one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38582b-e09f-4f68-9796-c32ce59b8f39",
   "metadata": {},
   "source": [
    "# Installations, imports, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfaf3b4-06e7-4b54-ad74-deeb5095e4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.33.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (4.33.0)\n",
      "Requirement already satisfied: accelerate==0.22.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (0.22.0)\n",
      "Requirement already satisfied: einops==0.6.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (0.6.1)\n",
      "Requirement already satisfied: langchain==0.0.300 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (0.0.300)\n",
      "Requirement already satisfied: xformers==0.0.21 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (0.0.21)\n",
      "Requirement already satisfied: bitsandbytes==0.41.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (0.41.1)\n",
      "Requirement already satisfied: sentence_transformers==2.2.2 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: chromadb==0.4.12 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (0.4.12)\n",
      "Requirement already satisfied: filelock in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from transformers==4.33.0) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from accelerate==0.22.0) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from accelerate==0.22.0) (2.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (3.9.0)\n",
      "Requirement already satisfied: anyio<4.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (3.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (0.6.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (0.0.92)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (2.8.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (1.10.15)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from langchain==0.0.300) (8.2.3)\n",
      "Requirement already satisfied: torchvision in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from sentence_transformers==2.2.2) (0.15.2a0)\n",
      "Requirement already satisfied: scikit-learn in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from sentence_transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: scipy in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from sentence_transformers==2.2.2) (1.10.1)\n",
      "Requirement already satisfied: nltk in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from sentence_transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (0.7.3)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (0.99.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (4.10.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (1.17.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (6.1.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (0.12.3)\n",
      "Requirement already satisfied: graphlib-backport>=1.0.3 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from chromadb==0.4.12) (1.1.0)\n",
      "Requirement already satisfied: sympy in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (68.2.2)\n",
      "Requirement already satisfied: wheel in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (3.29.2)\n",
      "Requirement already satisfied: lit in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (18.1.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from anyio<4.0->langchain==0.0.300) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.10.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.4)\n",
      "Requirement already satisfied: coloredlogs in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.15.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\n",
      "Requirement already satisfied: certifi in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from requests->transformers==4.33.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from requests->transformers==4.33.0) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from typer>=0.9.0->chromadb==0.4.12) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from typer>=0.9.0->chromadb==0.4.12) (13.7.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from importlib-resources->chromadb==0.4.12) (3.17.0)\n",
      "Requirement already satisfied: joblib in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from nltk->sentence_transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from torchvision->sentence_transformers==2.2.2) (10.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/amehmood/Research/ls/envs/R24env/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
    "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf07b232-8a0c-4d71-a1ee-25cd2d0c7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c658205-9ac0-49db-8a6e-fe11886a3f1f",
   "metadata": {},
   "source": [
    "## Initialize model, tokenizer, query pipeline\n",
    "Define the model, the device, and the bitsandbytes configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee645a48-22ca-46aa-bfc6-5e43060a1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 12:24:13.058883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 12:24:14.671528: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-06 12:24:15.017183: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-06 12:24:18.745050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-06 12:24:18.745133: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-06 12:24:18.745139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3552796c-162e-4534-a9b9-323bba378aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n",
    "# Model from Hugging Face hub\n",
    "#model_id  = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "model_id=\"meta-llama/Llama-2-7b-chat-hf\"       #GoodOne\n",
    "# model_id  = \"meta-llama/Llama-2-7b-chat-hf\"  \n",
    "# model_id  = \"4bit/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edef893-548d-4ca8-99ee-5b5c73718e2b",
   "metadata": {},
   "source": [
    "Prepare the model and the tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4e13f5-f3da-4cb2-99bb-4b9846ca4b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09908cbae15f4d6abe6711fcd72c25dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 82.936 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_2 = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0e003c-6f17-4e58-91c5-71b1bd7ccae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while loading the model: \n",
      "                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n",
      "                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
      "                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n",
      "                        `device_map` to `from_pretrained`. Check\n",
      "                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
      "                        for more details.\n",
      "                        \n",
      "Prepare model, tokenizer: 0.433 sec.\n"
     ]
    }
   ],
   "source": [
    "# #modified version of previous cell\n",
    "# from time import time\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# time_1 = time()\n",
    "# model_config = transformers.AutoConfig.from_pretrained(model_id)\n",
    "# # Assuming bnb_config is defined elsewhere\n",
    "# # Make sure bnb_config is appropriately configured for quantization\n",
    "\n",
    "# try:\n",
    "#     model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         trust_remote_code=True,\n",
    "#         config=model_config,\n",
    "#         quantization_config=bnb_config,\n",
    "#         device_map='auto',\n",
    "#     )\n",
    "# except ValueError as e:\n",
    "#     print(f\"Error occurred while loading the model: {e}\")\n",
    "#     # Handle the error appropriately, e.g., by using a different device or adjusting the configuration\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# time_2 = time()\n",
    "\n",
    "# print(f\"Prepare model, tokenizer: {round(time_2 - time_1, 3)} sec.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4c778-8c50-4c57-bcac-2451d364f803",
   "metadata": {},
   "source": [
    "Define the query pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b7390f-144f-4cd5-9e74-4c51c16ba544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.0 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ea5b6-9881-4660-b04a-aae908382dde",
   "metadata": {},
   "source": [
    "We define a function for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7f3513-4db7-4cc7-9d9e-24e61f4bba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=300,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd4bb7-9ece-404b-82a9-3c09705268d7",
   "metadata": {},
   "source": [
    "## Test the query pipeline\n",
    "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3a320e-ccd4-49de-bae6-d433fbab17ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 5.707 sec.\n",
      "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\n",
      "\n",
      "The State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, typically in January, in which the President reviews the current state of the union, outlines policy goals and proposals, and seeks to rally and inspire lawmakers and the public.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430a444-519d-4a67-9226-7a5916878662",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "## Check the model with a HuggingFace pipeline\n",
    "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9511bcd6-2758-4e0c-8765-b1997666a76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe State of the Union address is an annual speech given by the President of the United States to a joint session of Congress, typically in January or February of each year. The address provides an update on the state of the union, including the President's legislative agenda, major policy initiatives, and the current economic and political climate. The speech is meant to inform and rally the country, and is a key moment in the political calendar.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f2410-dbc6-403c-a116-cf871cdeed72",
   "metadata": {},
   "source": [
    "## Ingestion of data using Text loder\n",
    "We will ingest the newest presidential address, from Jan 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b4f3ec-52ca-4e30-8294-cb4bb1b1e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"./biden-sotu-2023-planned-official.txt\",\n",
    "                    encoding=\"utf8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b74c3-8138-4565-9e1a-cedd78523eb5",
   "metadata": {},
   "source": [
    "## Split data in chunks\n",
    "We split data in chunks using a recursive character text splitter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353b8f55-a80d-4ac5-a461-91693b8f4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=5)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    # Existing args\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4affda04-e473-45a3-a739-bc3584988168",
   "metadata": {},
   "source": [
    "## Creating Embeddings and Storing in Vector Store\n",
    "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9bcb868-c503-45a1-a8c0-7a0920096411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96493148-4f3f-46c4-8fa5-b6b2368cdb01",
   "metadata": {},
   "source": [
    "\n",
    "# Initializing ChromaDB \n",
    "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7efbe75-dfe4-45ff-875b-9cd9c9ce9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073a2a6-ca25-496b-a79e-0f73e2239747",
   "metadata": {},
   "source": [
    "## Initialize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75214f34-d8a4-479e-be66-14c0723d9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a270c4f-7f6d-480c-abd9-87333beebb8b",
   "metadata": {},
   "source": [
    "## Test the Retrieval-Augmented Generation\n",
    "We define a test function, that will run the query and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a6ffdcd-71cc-4fb2-958c-3315a88cc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag(qa, query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    time_1 = time()\n",
    "    result = qa.run(query)\n",
    "    time_2 = time()\n",
    "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
    "    print(\"\\nResult: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "255ea3cf-0e75-4f0e-a581-23d2b7aa59e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 7.222 sec.\n",
      "\n",
      "Result:   The nation's economic status is strong and prosperous, built on the foundation of freedom, fairness, and opportunity. The nation has faced challenges, but has always turned them into opportunities for growth and development. The future is within grasp, with the potential for even greater prosperity and success.\n",
      "\n",
      "Unhelpful Answer: I don't know. I'm not sure about the nation's economic status. I don't have any information on that.\n"
     ]
    }
   ],
   "source": [
    "test_rag(qa=qa, query=\"What is the nation economic status? Summarize. Keep it under 200 words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b431b-e113-4e8d-9351-4b75835c6033",
   "metadata": {},
   "source": [
    "Let's check few queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b93cf0c-9bc1-4150-89e3-7a35556be033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 8.443 sec.\n",
      "\n",
      "Result:   The main topics in the 2023 State of the Union address were unity, stability, and optimism for the future of America. The speaker emphasized the importance of seeing each other as fellow Americans and remembering the nation's founding ideals of equality and potential. The speaker also highlighted the strength of the nation's people, backbone, and soul, and expressed optimism about the future of America as long as the nation works together. The speaker also mentioned the importance of God's blessing and protection for the troops.\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca254473-955c-4c09-929d-787785bfc050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 12.837 sec.\n",
      "\n",
      "Result:   The economic status of the nation is a crucial aspect of the State of the Union address. The President highlights the nation's economic progress, challenges, and goals. In the address, the President notes that the nation has built the strongest, freest, and most prosperous nation the world has ever known, but acknowledges that the current times are hard. Despite these challenges, the President expresses optimism about America's future, citing the nation's ability to turn every crisis into an opportunity. The President also emphasizes the importance of protecting freedom and liberty, expanding fairness and opportunity, and saving democracy. Overall, the President's focus on the nation's economic status underscores the importance of a strong and prosperous economy in ensuring the nation's continued growth and success.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45a9d0-01c7-4c5a-831f-77c7525a596b",
   "metadata": {},
   "source": [
    "# Document sources\n",
    "Let's check the documents sources, for the last query run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a6a3542-bf99-4269-adc1-52e117d71dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "Retrieved documents: 4\n",
      "Source:  ./SOUT1.txt\n",
      "Text:  And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.  \n",
      "\n",
      "We will meet the test. \n",
      "\n",
      "To protect freedom and liberty, to expand fairness and opportunity. \n",
      "\n",
      "We will save democracy. \n",
      "\n",
      "As hard as these times have been, I am more optimistic about America today than I have been my whole life. \n",
      "\n",
      "Because I see the future that is within our grasp. \n",
      "\n",
      "Because I know there is simply nothing beyond our capacity. \n",
      "\n",
      "We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n",
      "\n",
      "The only nation that can be defined by a single word: possibilities. \n",
      "\n",
      "So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n",
      "\n",
      "Source:  ./SOUT1.txt\n",
      "Text:  And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.  \n",
      "\n",
      "We will meet the test. \n",
      "\n",
      "To protect freedom and liberty, to expand fairness and opportunity. \n",
      "\n",
      "We will save democracy. \n",
      "\n",
      "As hard as these times have been, I am more optimistic about America today than I have been my whole life. \n",
      "\n",
      "Because I see the future that is within our grasp. \n",
      "\n",
      "Because I know there is simply nothing beyond our capacity. \n",
      "\n",
      "We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n",
      "\n",
      "The only nation that can be defined by a single word: possibilities. \n",
      "\n",
      "So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n",
      "\n",
      "Source:  ./SOUT1.txt\n",
      "Text:  And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.  \n",
      "\n",
      "We will meet the test. \n",
      "\n",
      "To protect freedom and liberty, to expand fairness and opportunity. \n",
      "\n",
      "We will save democracy. \n",
      "\n",
      "As hard as these times have been, I am more optimistic about America today than I have been my whole life. \n",
      "\n",
      "Because I see the future that is within our grasp. \n",
      "\n",
      "Because I know there is simply nothing beyond our capacity. \n",
      "\n",
      "We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n",
      "\n",
      "The only nation that can be defined by a single word: possibilities. \n",
      "\n",
      "So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n",
      "\n",
      "Source:  ./SOUT1.txt\n",
      "Text:  And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.  \n",
      "\n",
      "We will meet the test. \n",
      "\n",
      "To protect freedom and liberty, to expand fairness and opportunity. \n",
      "\n",
      "We will save democracy. \n",
      "\n",
      "As hard as these times have been, I am more optimistic about America today than I have been my whole life. \n",
      "\n",
      "Because I see the future that is within our grasp. \n",
      "\n",
      "Because I know there is simply nothing beyond our capacity. \n",
      "\n",
      "We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n",
      "\n",
      "The only nation that can be defined by a single word: possibilities. \n",
      "\n",
      "So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = vectordb.similarity_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d4129-9a45-40d9-8c0d-b5ae50acaa71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e06228-0cc2-4a2f-9682-4302e3392264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
