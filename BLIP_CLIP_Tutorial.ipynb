{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e48207-8998-44d4-b79f-ca28f2e346b3",
   "metadata": {},
   "source": [
    "### A Tutorial on BLIP and CLIP Models: Harnessing Visual and Textual Intelligence\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "The fusion of computer vision and natural language processing (NLP) has significantly advanced with the advent of models like CLIP (Contrastive Language–Image Pre-training) and BLIP (Bootstrapped Language-Image Pretraining). These models represent a leap in machine understanding of images and texts by allowing them to connect and analyze these domains simultaneously. They offer capabilities such as image-text retrieval, visual reasoning, and multimodal embeddings, which have a wide range of applications—from image search engines to visual question answering.\n",
    "\n",
    "In this tutorial, we will explore how to use both CLIP and BLIP models from Hugging Face's transformers library, highlighting their unique architectures and how to integrate them into your projects.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Understanding CLIP (Contrastive Language–Image Pre-training)\n",
    "\n",
    "##### 1.1 What is CLIP?\n",
    "\n",
    "CLIP, developed by OpenAI, aims to understand images and textual descriptions in a shared space. Unlike conventional models trained on classification labels, CLIP is trained using a contrastive objective where the goal is to align images with their textual descriptions and vice versa. This allows CLIP to perform tasks such as zero-shot image classification, image captioning, and more without being explicitly trained for these tasks.\n",
    "\n",
    "##### 1.2 How CLIP Works\n",
    "\n",
    "CLIP is pre-trained on a diverse dataset of images and their associated texts (captions). The architecture consists of two main parts:\n",
    "- A visual encoder (typically based on Vision Transformer (ViT) or ResNet) processes images.\n",
    "- A text encoder (based on Transformer) processes the text.\n",
    "These embeddings are then projected into a joint space where their similarities are maximized if the image and text correspond to one another.\n",
    "\n",
    "##### 1.3 Using CLIP in Practice\n",
    "\n",
    "To demonstrate the power of CLIP, let’s use it to classify an image by predicting its similarity to a set of textual descriptions.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load an image (local path or from the web)\n",
    "image_path = \"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/training_set/dogs/dog.64.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Prepare the text descriptions\n",
    "texts = [\"a dog\", \"a cat\", \"an airplane\", \"a logo\"]\n",
    "\n",
    "# Process inputs\n",
    "inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get CLIP predictions\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "# Display predictions\n",
    "print(\"CLIP Predictions:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Probability of '{text}': {probs[0][i].item():.4f}\")\n",
    "```\n",
    "\n",
    "Here, the CLIP model takes an image of a dog and attempts to classify it by computing the similarity between the image and the text descriptions. CLIP’s zero-shot capability allows it to perform classification tasks even without task-specific training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Understanding BLIP (Bootstrapped Language-Image Pretraining)\n",
    "\n",
    "##### 2.1 What is BLIP?\n",
    "\n",
    "BLIP, developed by Salesforce Research, introduces a new way to align visual and textual modalities for image captioning and question answering tasks. BLIP leverages self-supervised learning to bootstrap from noisy web data, making it particularly effective in learning fine-grained image-text relationships. BLIP is designed to handle multimodal tasks such as caption generation, question-answering, and grounded image-text representations.\n",
    "\n",
    "##### 2.2 How BLIP Works\n",
    "\n",
    "BLIP's architecture consists of:\n",
    "- **A Vision Transformer (ViT)** that processes images.\n",
    "- **A BERT-based text encoder** that processes text.\n",
    "BLIP aligns the embeddings of the image and text representations by minimizing the contrastive loss, similar to CLIP, but BLIP is further optimized for tasks such as image captioning.\n",
    "\n",
    "##### 2.3 Using BLIP in Practice\n",
    "\n",
    "Below is a simple example of how to use BLIP to generate captions for an image:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load an image\n",
    "image_path = \"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/training_set/dogs/dog.64.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Prepare the image for BLIP\n",
    "inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the generated caption\n",
    "print(f\"BLIP Caption: {caption}\")\n",
    "```\n",
    "\n",
    "Here, the BLIP model takes an image and generates a descriptive caption. This capability is particularly useful in real-world applications such as automated content generation, social media, and more.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Comparison Between CLIP and BLIP\n",
    "\n",
    "| Aspect                  | CLIP                                                | BLIP                                                 |\n",
    "|-------------------------|-----------------------------------------------------|------------------------------------------------------|\n",
    "| **Developed By**         | OpenAI                                              | Salesforce Research                                  |\n",
    "| **Primary Task**         | Zero-shot image classification, image-text retrieval| Image captioning, visual question answering          |\n",
    "| **Architecture**         | Vision Transformer + Text Transformer               | Vision Transformer + BERT-based Text Encoder         |\n",
    "| **Training**             | Contrastive learning on image-text pairs            | Contrastive learning + supervised image captioning   |\n",
    "| **Use Cases**            | Classification, multimodal search, zero-shot tasks  | Caption generation, image-text reasoning             |\n",
    "| **Pre-training Dataset** | Diverse internet-based dataset (image-text pairs)   | Large-scale web data (image-text pairs)              |\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Applications of CLIP and BLIP\n",
    "\n",
    "- **Content Moderation**: CLIP can identify inappropriate or harmful content by aligning text-based rules with images, making content filtering more robust.\n",
    "- **Image Search**: Both CLIP and BLIP can be used to perform reverse image searches and content-based image retrieval.\n",
    "- **Automated Captioning**: BLIP's ability to generate captions makes it ideal for automatically generating metadata for images or improving accessibility for the visually impaired.\n",
    "- **Visual Question Answering**: BLIP can be used in applications where answering questions about visual content is needed (e.g., customer support).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Conclusion\n",
    "\n",
    "CLIP and BLIP represent the cutting edge in aligning visual and textual modalities. Whether for zero-shot classification, caption generation, or complex multimodal tasks, these models offer powerful, flexible tools for understanding the relationship between images and text. As both research and industry increasingly explore multimodal AI, CLIP and BLIP stand out as two of the most promising technologies driving these advances.\n",
    "\n",
    "By integrating CLIP and BLIP into your workflows, you can build applications that understand, describe, and interact with the world in ways previously unimaginable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. References\n",
    "\n",
    "1. Radford, A., et al. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. OpenAI. Available at: https://openai.com/research/clip\n",
    "2. Li, J., et al. (2022). *BLIP: Bootstrapped Language-Image Pre-training for Unified Vision-Language Understanding and Generation*. Salesforce Research. Available at: https://arxiv.org/abs/2201.12086\n",
    "3. Hugging Face Documentation. *Transformers Library for CLIP and BLIP*. Available at: https://huggingface.co/models\n",
    "4. Ultralytics YOLOv5 GitHub. Available at: https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de718f5f-e144-4350-99db-584607ad879b",
   "metadata": {},
   "source": [
    "## CLIP Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc12123-b26e-4fde-85dd-e3d9fa049445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 15:43:31.829741: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 15:43:31.831054: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 15:43:31.856370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 15:43:33.981893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Predictions:\n",
      "Probability of 'a dog': 0.9937\n",
      "Probability of 'a cat': 0.0044\n",
      "Probability of 'an airplane': 0.0001\n",
      "Probability of 'a logo': 0.0018\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load an image (local path or from the web)\n",
    "image_path = \"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/training_set/dogs/dog.64.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Prepare the text descriptions\n",
    "texts = [\"a dog\", \"a cat\", \"an airplane\", \"a logo\"]\n",
    "\n",
    "# Process inputs\n",
    "inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get CLIP predictions\n",
    "with torch.no_grad():\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "# Display predictions\n",
    "print(\"CLIP Predictions:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Probability of '{text}': {probs[0][i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46ffd1-3f54-402d-8e83-79e0f0d0296d",
   "metadata": {},
   "source": [
    "### BLIP Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d171c875-a0de-4b70-b053-6a5f92d41746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amehmood/anaconda3/envs/main_env/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Caption: a dog is sitting in the grass with its owner\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load an image\n",
    "image_path = \"/mnt/d/FY2024/DataSet2024/dog vs cat/dataset/training_set/dogs/dog.64.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Prepare the image for BLIP\n",
    "inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the generated caption\n",
    "print(f\"BLIP Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4980c-3f72-4f39-9bd9-e0a451758240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
